---
layout: post
title: 【翻译】图形渲染管线之旅，第一章
categories: [blog ]
tags: [Graphics, ]
description: 图形渲染管线之旅。
header-img: "./../images/gta-v.jpg"
---

|原文|[A Trip Through the Graphics Pipeline, part 1](https://fgiesen.wordpress.com/2011/07/01/a-trip-through-the-graphics-pipeline-2011-part-1/)|
|----|----|
|作者|[Fabian "ryg" Giesen](https://fgiesen.wordpress.com/about/)|
|翻译|PandaChen|
|进度|90.0%|

>这篇文章是"图形学渲染管线之旅"系列文章的一部分。光看的话看不进去，所以权当做个搬运和翻译，看心情更，看心情翻……如果有任何问题请留言/邮件指正

自从我在这里发表一些文章以来已经有一段时间过去了，我琢磨着我可以在这里解释一些到2011年为止图形硬件和软件的一些基本概念。你可以找到一些比较实用的关于图形栈（graphics stack）到底在你的PC里干了啥的介绍，但是一般不会涉及它们是"如何做到"或者"为什么要这样"。我会试着在不那么具体地涉及到任何硬件部分的基础上填补一下这一部分的空白。我主要会围绕在windows上运行D3D9/10/11的DX11系列硬件来讲解，因为那是我比较熟悉的PC栈。不过关于API这些东西的细节并不会非常影响你阅读这一部分，而一旦我们开始真正接触GPU，那么就都是一些本地指令了（native commands）。

## 应用程序

这是你自己写的代码，这也是你自己写出来的bug。真的，就算API运行时和驱动确实有bug，你的那些问题也不会是由它们造成的。现在快去修复这些bug！

## API运行时

你通过API来创建资源、设置状态、调用命令。API运行时会追踪你应用设置的状态、验证参数以及做一些错误或者一致性检查，管理一些用户可见的资源，有可能还会验证一下shader的代码或是链接shader（至少D3D会这么做，在OpenGL中则由驱动层来处理），也许还会做一些更多的工作，然后会将其交付给图形驱动——或者更确切一点——用户模式驱动（user-mode driver）。

## 用户模式图形驱动（The User-mode graphics driver, or UMD）

大部分CPU端做的一些神奇的事情就是在这个阶段发生的。而且如果你的应用因为某些API的原因而崩溃了，那么通常也是在这个阶段。它叫做"nvd3dum.dll"（NVidia）或者是"atiumd*.dll"（AMD）。正如其名，这是用户模式下的代码，它也运行在和你的应用（以及API运行时）相同的上下文和地址空间中，也没有任何更高级的权限。它实现了D3D调用的一些更低层级的API（DDI）部分。这部分的API基本上和你在表面上看到的差不多，但是会更明确直接地处理事情，比如内存管理之类的。

编译shader之类的事情也是在这个部分发生的。D3D将验证过的，有效的shader令牌流（shader token stream）传给UMD——即是说，它已经检查过代码是有效的，语法也是正确且在D3D的限制之内的（即使用了正确的类型，没有超出可用的纹理/采样器，没有使用过多的常量缓冲等等）。它们从HLSL代码编译而来，而且通常还会有大量的高级层面的优化（循环优化、无效代码清除、常量的传递、分支预测等）——这是好事，意味着这些相关的高昂的优化都会在编译时执行，而驱动会从中受益。不过，还有一些低层级的优化（分配寄存器、循环展开等），驱动会更倾向于由自己来处理。简而言之，这一部分会很快地转换为一些中间形式（intermediate representation）并且被进一步地编译。shader硬件指令已经相当接近D3D的字节码，编译不需要再做很牛逼的工作来给出一个比较好的结果（HLSL的编译器已经做了一些有用的高级且高昂的优化），但是依然有许多底层的细节是D3D不知道也不关心的，它们也不是一些简单的步骤。

当然，如果你的应用是一个足够知名的游戏的话，NV和AMD的程序员们很可能会来看看你写的shader，然后针对他们的硬件做一些人工的优化——当然啦它们最好能产生相同的结果。这一部分的shader也会被检测并被UMD取代。

更有意思的是，有些API的状态最后是会被编译成为一个shader的，举个栗子，相对来说比较独立的（或是极少使用的）特性——比如texture borders，有可能不会用纹理采样器（texture sampler）来实现，而是用额外的代码放在shader中来模拟。这意味着对于同一份shader，不同的API状态的组合可能会产生不同的版本。

另外，这也是为什么在第一次使用新的shader或者资源的时候会产生延迟的原因。许多创建/编译的工作也被驱动推迟了，仅仅在真正需要的时候执行（你不会相信一些应用究竟创建了多少没用到的东西的）。图形程序员们表示，如果你想要确保一个东西被真正创建了（而不仅仅只是分配了内存），你需要一些调用一些冗余的指令来“激活”它们。虽然有点烦而且很丑，但是从笔者1999年开始用3D硬件的时候就一直是这样的了，你还是多习惯习惯吧。

好了咱们继续。UMD同样需要处理一些有意思的东西，像是D3D9所有“遗留”下来的shader版本，以及固定功能管线——所有这些功能都会D3D执行。3.0版本的shader profile并不那么差（实际上还是挺靠谱的），但是2.0版本的就很混乱了，各种1.x的版本则更不怎么样，还记得1.3版本的像素着色器吗？或者说，带有顶点光照和一些别的功能的固定功能顶点管线？是的，D3D和现代显卡仍保留着对所有这些功能的支持，虽然他们只是将其转换为现在的shader版本（他们已经这么做很久了）。

接下来就涉及到了内存管理这一类东西。UMD会接受类似创建纹理（texture creation）这些命令，然后需要给他们分配显存空间。实际上，UMD仅仅是把从KMD（kernel-mode driver）拿来的一些大块儿的显存块进行进一步的分配。实际上页映射的操作（以及管理显存的哪部分是UMD可见的，和GPU可以访问内存的哪部分等）是KM的权利，UMD是做不到的。

但是UMD可以做一些比如[纹理重组（swizzling textures）](https://fgiesen.wordpress.com/2011/01/17/texture-tiling-and-swizzling/)（除非GPU可以在硬件层面上自己完成，通常是使用2D的块单元而不是真正的3D管线）和调度内存和显存之间的传输这一类事情。更重要的是，一旦KMD进行了分配好了内存，他就能够写入指令缓冲（command buffers）（或者说DMA缓冲——我会交替使用这两个名字）。你所有的状态转换，绘制操作都会被UMD转化为硬件能懂的命令。这样就有许多操作不需要你手工参与，比如将纹理和shader上传到显存。

总的来说，驱动会将尽量多的实际操作放到UMD中。UMD是用户模式下的代码，所以运行在这里的任何东西都不需要昂贵的kernel-mode转换，它可以自由地分配内存，分派工作到多线程，等等——相当于一个常规的DLL而已（即便它是由API加载的，而不是由你的app直接调用的）。这对驱动的开发也有好处——如果UMD崩溃了，应用也会跟着崩溃，但是整个系统不会，它也可以在系统运行的同时被替换（因为只是个DLL而已！），它也可以用调试器来debug，等等。所以它不仅仅高效，而且方便。

但是到目前为止，我还未提及到房间里面存在的那只大象。

我是不是说了“user-mode driver"来着？实际上，我说的是”user-mode drivers“。就像我之前说的，UMD仅仅是个DLL，好吧，它能通过D3D的帮助直接通向KMD，但它仍然是个常规的DLL，当被调用的时候运行在普通的地址空间。

但是我们已经用多任务的OS很久了。

而我一直谈论的这个“GPU”，是一个被共享的资源。只有一个GPU驱动着你主显示器（即使你使用了SLI/Crossfire
）。但是我们却有不同的应用试图访问它（并且假装他们是唯一在做这件事的应用）。这个工作当然不是自动完成的。在过去的日子里，解决方案就是一次只把3D处理器给一个应用，当那个应用激活的时候，其它的app就无法访问GPU了。但是这并没有实际地解决这个问题，因此我们会需要一些组件来调度GPU的访问，分配时间片等等工作。

## 走进调度器

这是一个系统组件——需要注意的是这是GPU的调度器，而不是CPU或者IO的调度器。它做的完全就像你想的那样——通过给不同的应用分配时间片的方式仲裁对3D管线的访问。期间会发生上下文的切换会导致GPU上的一些状态的改变（会给指令缓冲产生额外的命令），而且显存也可能会换进或者换出一些资源。当然，任意给定的时刻只能有一个进程能够真正把命令提交给3D管线。

你会经常发现主机程序员抱怨他们没有PC上面的那些相当高级或是自动化的3D API，以及它们所带来的一些性能损耗。但是PC上的3D API/驱动有一些比主机游戏更加复杂的问题需要解决——例如他们需要跟踪当前全部的状态，因为随时可能会有人发现一些底层的问题。他们也需要在一些差劲的应用上做工作，试着去修复它们背后的性能问题。这相当烦人，也没有人会喜欢，尤其是驱动作者们自己，然而这就是商业上成功的地方。人们希望能有持续（并且平滑）运行的东西。你对着应用喊着“可那是错的！”，然后郁闷地慢慢检查，是得不到任何朋友的。

不管怎样，我们继续，下一站：kernel mode

## 内核模式驱动（Kernel-mode driver, KMD）

这一部分是真正处理硬件的部分。在同一时间可能有许多UMD实例在运行，但是永远只有一个KMD，而且如果这个崩了，那你就真的炸了——以前会是蓝屏错误，但是现在windows已经知道如何去终止一个奔溃的驱动并重载它了。只要它只是一个崩溃，而不是什么内核内存的损坏，否则的话所有的努力都白费了。

KMD对所有的东西都只处理一次。即便有许多不同的应用在抢占，这里仍然只有一个GPU显存。需要有一个人来做主，分配以及映射实际的物理内存。同样的，得有个人来初始化GPU，设置显示模式（并且从显示设备获取更多的信息），管理光标，设定HW监视定时器来让GPU在一定时间内没有响应的时候重启等等。这些就是KMD做的。

这里还有一个叫DRM的东西，它会在视频播放器和GPU之间设置一个保护通道，所以视频解码后珍贵的像素对再牛逼的用户代码也是不可见的，他们无法做一些把视频烧录到光盘上这些事情。这个过程中KMD也有参与。

对我们来说更重要的是，KMD管理着实际的指令缓冲，即是硬件真正使用的那部分。UMD并不和真正的指令缓冲进行交互，实际上它们只是GPU可访问内存中随机的片段。实际上UMD处理完这些“指令缓冲”之后，递交给调度器，然后就等待被处理的时候，将这些内容传给KMD。然后KMD才将这些“指令缓冲”的内容写入主要的指令缓冲中，而且根据GPU命令处理器能否从主存中读数据，它可能会需要先将其通过DMA传到显存中。主要的那个指令缓冲通常是一个[环形缓冲区（ring buffer）](https://fgiesen.wordpress.com/2010/12/14/ring-buffers-and-queues/)，写在这里的人和东西都是系统/初始化的命令，调用一些其他实际的3D指令缓冲。

但到目前为止这依然是内存中的缓冲。它的位置图像处理器是已知的——通常有个读指针，表示GPU在主要的指令缓冲中读到哪儿了，以及一个写指针，表示KMD目前为止写到缓冲的哪儿了（或者更确切地说，它告诉GPU它已经写了多远。这些都是硬件上的寄存器，而且是与内存映射的——KMD定时地来更新它们（通常是它提交新的工作块的时候）。

## 总线

当然，写的过程并不是直接写入显卡的，它需要先经过总线——通常是PCI Express。DMA传输等等也是走的这条路。这并不会很花时间，但它仍是我们旅程的一个部分，直到——

## 指令处理器（command processor）

指令处理器是GPU的前端——这部分实际读取KMD写好的命令。我会在下一篇博文中再来讲解，因为这篇已经够长的了。

## 又及：OpenGL

OpenGL大部分和上面所讲的相同，除了它在API和UMD层之间没有明显的区别。另外不像D3D，GLSL的编译是不会交给API的处理的，全部由驱动执行。缺点是这里有多少显卡供应商就会有多少GLSL的前端，而他们全都实现相同的事情，有着各自的bug和特点。伐开森。这同时也意味着驱动在处理shader的时候需要自己完成全部的优化——包括那些昂贵的优化。针对这个问题，D3D的字节码在这里的表现就好得多——只有一个编译器（所以不同的厂商就没有不兼容的情况），并且它允许一些比你通常做的更昂贵的数据流分析。

## 遗漏及简化

这仅仅是一个概括，我省略了成吨的细节。比如说这里只有一个调度器，有不同的实现（驱动可以自行选择），我也没有解释CPU和GPU是的同步是如何处理的这个大问题，等等。我也可能会忘了一些很重要的问题——如果是这样的话，请告诉我，我会修正的。
